{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naveyah0607/vanilla-policy-gradient/blob/main/NavyaACTrainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUyBLnHl7R4X",
        "outputId": "7233d304-690e-4e8d-dbb9-bbabdb0be492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py install for box2d-py ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: box2d-py was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. pip 23.1 will enforce this behaviour change. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium==0.27.1 -q\n",
        "!pip install gymnasium[box2d] -q\n",
        "!pip install moviepy -q\n",
        "!pip install -U kora -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def apply_discount(raw_reward, gamma=0.99):\n",
        "    raw_reward.reverse()\n",
        "    discounted_rtg_reward = [sum(raw_reward[i:][::-1]*(gamma**np.arange(len(raw_reward[i:])))) for i in range(len(raw_reward))]\n",
        "    raw_reward.reverse()\n",
        "    discounted_rtg_reward.reverse()\n",
        "    discounted_rtg_reward = np.array(discounted_rtg_reward)\n",
        "    discounted_rtg_reward = discounted_rtg_reward - np.mean(discounted_rtg_reward) / (np.std(discounted_rtg_reward) + np.finfo(np.float32).eps)\n",
        "    return torch.tensor(discounted_rtg_reward, dtype=torch.float32, device=get_device())"
      ],
      "metadata": {
        "id": "dCtXviQK7f5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/Colab Notebooks/videos\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b35ryyE8A3c",
        "outputId": "b2a834ac-a89f-4875-bd14-94b7b66da000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks/videos\n",
            "CartPole\t    CartPole_v1_t2.pkl\tLunarLander\n",
            "CartPole_v1_t0.pkl  CartPole_v1_t3.pkl\tLunarLander_v2_t0.pkl\n",
            "CartPole_v1_t1.pkl  CartPole_v1_t4.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import pickle\n",
        "import random\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "from collections import deque, namedtuple\n",
        "from gymnasium.utils.save_video import save_video\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "\n",
        "# Class for training an RL agent with Actor-Critic\n",
        "class ACTrainer:\n",
        "    def __init__(self, params):\n",
        "        self.params = params\n",
        "        self.env = gym.make(self.params['env_name'])\n",
        "        self.agent = ACAgent(env=self.env, params=self.params)\n",
        "        self.actor_net = ActorNet(input_size=self.env.observation_space.shape[0], output_size=self.env.action_space.n, hidden_dim=self.params['hidden_dim']).to(get_device())\n",
        "        self.critic_net = CriticNet(input_size=self.env.observation_space.shape[0], output_size=1, hidden_dim=self.params['hidden_dim']).to(get_device())\n",
        "        self.actor_optimizer = Adam(params=self.actor_net.parameters(), lr=self.params['actor_lr'])\n",
        "        self.critic_optimizer = Adam(params=self.critic_net.parameters(), lr=self.params['critic_lr'])\n",
        "        self.trajectory = None\n",
        "\n",
        "    def run_training_loop(self):\n",
        "        list_ro_reward = list()\n",
        "        for ro_idx in range(self.params['n_rollout']):\n",
        "            self.trajectory = self.agent.collect_trajectory(policy=self.actor_net)\n",
        "            self.update_critic_net()\n",
        "            self.estimate_advantage()\n",
        "            self.update_actor_net()\n",
        "            sum_of_rewards = 0\n",
        "            reward_list = self.trajectory['reward']\n",
        "            for trajectory_reward_list in reward_list:\n",
        "                sum_of_rewards += torch.tensor(np.sum(trajectory_reward_list), dtype=torch.float32, device=get_device())  \n",
        "            avg_ro_reward = (sum_of_rewards/len(reward_list)).item()\n",
        "            print(f'End of rollout {ro_idx}: Average trajectory reward is {avg_ro_reward: 0.2f}')\n",
        "            # Append average rollout reward into a list\n",
        "            list_ro_reward.append(avg_ro_reward)\n",
        "        # Save avg-rewards as pickle files\n",
        "        pkl_file_name = self.params['exp_name'] + '.pkl'\n",
        "        with open(pkl_file_name, 'wb') as f:\n",
        "            pickle.dump(list_ro_reward, f)\n",
        "        # Save a video of the trained agent playing\n",
        "        self.generate_video()\n",
        "        # Close environment\n",
        "        self.env.close()\n",
        "\n",
        "    def update_critic_net(self):\n",
        "        for critic_iter_idx in range(self.params['n_critic_iter']):\n",
        "            self.update_target_value()\n",
        "            for critic_epoch_idx in range(self.params['n_critic_epoch']):\n",
        "                critic_loss = self.estimate_critic_loss_function()\n",
        "                critic_loss = critic_loss.detach().clone()\n",
        "                # critic_loss.backward()\n",
        "                self.critic_optimizer.step()\n",
        "                self.critic_optimizer.zero_grad()\n",
        "\n",
        "    def update_target_value(self, gamma=0.99):\n",
        "        obs_list =  self.trajectory['obs']\n",
        "        reward_list = self.trajectory['reward']\n",
        "        state_values = [self.critic_net.forward(obs) for obs in obs_list]\n",
        "        next_state_value = 0.0\n",
        "        target_values = []\n",
        "        for i in reversed(range(len(obs_list))):\n",
        "            reward = torch.tensor(reward_list[i]).unsqueeze(1)\n",
        "            target_value = reward + gamma * state_values[i]\n",
        "            target_values.insert(0, target_value)\n",
        "        self.trajectory['state_value'] = state_values\n",
        "        self.trajectory['target_value'] = target_values\n",
        "\n",
        "\n",
        "    \n",
        "    def estimate_advantage(self, gamma=0.99):\n",
        "        advantages = []\n",
        "        for state_value, target_value in zip(self.trajectory['state_value'], self.trajectory['target_value']):\n",
        "    # compute advantage for current element\n",
        "            advantage = target_value - state_value\n",
        "            advantages.append(advantage)\n",
        "\n",
        "# concatenate list of tensors into a single tensor\n",
        "        advantages = torch.cat(advantages, dim=0)\n",
        "\n",
        "# assign computed advantages to trajectory dictionary\n",
        "        self.trajectory['advantage'] = advantages\n",
        "\n",
        "\n",
        "    def update_actor_net(self):\n",
        "        actor_loss = self.estimate_actor_loss_function()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "\n",
        "    def estimate_critic_loss_function(self):\n",
        "        critic_losses = []\n",
        "\n",
        "# iterate over each observation in the batch\n",
        "        for state_value, target_value in zip(self.trajectory['state_value'], self.trajectory['target_value']):\n",
        "    # compute the critic loss for this observation using mean squared error\n",
        "            \n",
        "            \n",
        "            critic_loss = torch.nn.functional.mse_loss(state_value.float(), target_value.float())\n",
        "    # append the critic loss to the list of losses\n",
        "        critic_losses.append(critic_loss)\n",
        "\n",
        "# compute the mean of the critic losses for the entire batch\n",
        "        critic_loss = torch.mean(torch.stack(critic_losses))\n",
        "        return critic_loss\n",
        "\n",
        "    def estimate_actor_loss_function(self):\n",
        "        actor_loss = [torch.mean(-self.trajectory['log_prob'][t_idx] * apply_discount(self.trajectory['advantage'][t_idx].tolist())) for t_idx in range(self.params['n_trajectory_per_rollout'])]\n",
        "        return torch.mean(torch.stack(actor_loss))\n",
        "\n",
        "\n",
        "    def generate_video(self, max_frame=1000):\n",
        "        self.env = gym.make(self.params['env_name'], render_mode='rgb_array_list')\n",
        "        obs, _ = self.env.reset()\n",
        "        for _ in range(max_frame):\n",
        "            action_idx, log_prob = self.actor_net(torch.tensor(obs, dtype=torch.float32, device=get_device()))\n",
        "            obs, reward, terminated, truncated, info = self.env.step(self.agent.action_space[action_idx.item()])\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "        save_video(frames=self.env.render(), video_folder=self.params['env_name'][:-3], fps=self.env.metadata['render_fps'], step_starting_index=0, episode_index=0)\n",
        "\n",
        "\n",
        "class ActorNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim):\n",
        "        super(ActorNet, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_size),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, obs):\n",
        "        action_probs = self.network(obs)\n",
        "        dist = Categorical(action_probs)\n",
        "        action_index = dist.sample()\n",
        "        log_prob = dist.log_prob(action_index)\n",
        "        return action_index, log_prob\n",
        "\n",
        "\n",
        "class CriticNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim):\n",
        "        super(CriticNet, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, obs):\n",
        "        return self.network(obs)\n",
        "\n",
        "\n",
        "# Class for agent\n",
        "class ACAgent:\n",
        "    def __init__(self, env, params=None):\n",
        "        self.env = env\n",
        "        self.params = params\n",
        "        self.action_space = [action for action in range(self.env.action_space.n)]\n",
        "\n",
        "    def collect_trajectory(self, policy):\n",
        "        obs, _ = self.env.reset(seed=self.params['rng_seed'])\n",
        "        rollout_buffer = list()\n",
        "        for _ in range(self.params['n_trajectory_per_rollout']):\n",
        "            trajectory_buffer = {'obs': list(), 'log_prob': list(), 'reward': list()}\n",
        "            while True:\n",
        "                obs = torch.tensor(obs, dtype=torch.float32, device=get_device())\n",
        "                # Save observation\n",
        "                trajectory_buffer['obs'].append(obs)\n",
        "                action_idx, log_prob = policy(obs)\n",
        "                obs, reward, terminated, truncated, info = self.env.step(self.action_space[action_idx.item()])\n",
        "                # Save log-prob and reward into the buffer\n",
        "                trajectory_buffer['log_prob'].append(log_prob)\n",
        "                trajectory_buffer['reward'].append(reward)\n",
        "                # Check for termination criteria\n",
        "                if terminated or truncated:\n",
        "                    obs, _ = self.env.reset()\n",
        "                    rollout_buffer.append(trajectory_buffer)\n",
        "                    break\n",
        "        rollout_buffer = self.serialize_trajectory(rollout_buffer)\n",
        "        return rollout_buffer\n",
        "\n",
        "    # Converts a list-of-dictionary into dictionary-of-list\n",
        "    @staticmethod\n",
        "    def serialize_trajectory(rollout_buffer):\n",
        "        serialized_buffer = {'obs': list(), 'log_prob': list(), 'reward': list()}\n",
        "        for trajectory_buffer in rollout_buffer:\n",
        "            serialized_buffer['obs'].append(torch.stack(trajectory_buffer['obs']))\n",
        "            serialized_buffer['log_prob'].append(torch.stack(trajectory_buffer['log_prob']))\n",
        "            serialized_buffer['reward'].append(trajectory_buffer['reward'])\n",
        "        return serialized_buffer"
      ],
      "metadata": {
        "id": "b8FVqJjt8Or1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      # set simulation parameters\n",
        "params_list = [{\n",
        "    'env_name': 'LunarLander-v2',\n",
        "    'rng_seed': 6369,\n",
        "    'n_rollout': 100,\n",
        "    'n_trajectory_per_rollout': 60,\n",
        "    'n_critic_iter': 20,\n",
        "    'n_critic_epoch': 20,\n",
        "    'hidden_dim': 128,\n",
        "    'actor_lr': 3e-3,\n",
        "    'critic_lr': 3e-4,\n",
        "    'exp_name': 'LunarLander_v2_t2'\n",
        "},\n",
        "{\n",
        "    'env_name': 'LunarLander-v2',\n",
        "    'rng_seed': 6369,\n",
        "    'n_rollout': 100,\n",
        "    'n_trajectory_per_rollout': 60,\n",
        "    'n_critic_iter': 10,\n",
        "    'n_critic_epoch': 10,\n",
        "    'hidden_dim': 128,\n",
        "    'actor_lr': 3e-3,\n",
        "    'critic_lr': 3e-4,\n",
        "    'exp_name': 'LunarLander_v2_t1'\n",
        "},\n",
        "\n",
        "{\n",
        "    'env_name': 'LunarLander-v2',\n",
        "    'rng_seed': 6369,\n",
        "    'n_rollout': 100,\n",
        "    'n_trajectory_per_rollout': 60,\n",
        "    'n_critic_iter': 1,\n",
        "    'n_critic_epoch': 1,\n",
        "    'hidden_dim': 128,\n",
        "    'actor_lr': 3e-3,\n",
        "    'critic_lr': 3e-4,\n",
        "    'exp_name': 'LunarLander_v2_t0'\n",
        "},\n",
        "]\n",
        "\n",
        "# Train agent with each set of parameters\n",
        "for params in params_list:\n",
        "    # Seed RNGs\n",
        "    seed_everything(params['rng_seed'])\n",
        "# Train agent\n",
        "    trainer = ACTrainer(params)\n",
        "    trainer.run_training_loop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn2pIW-a8O1n",
        "outputId": "b8d3b559-7066-49ed-bdc4-4fe7829cf36f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of rollout 0: Average trajectory reward is -179.39\n",
            "End of rollout 1: Average trajectory reward is -195.57\n",
            "End of rollout 2: Average trajectory reward is -178.27\n",
            "End of rollout 3: Average trajectory reward is -162.44\n",
            "End of rollout 4: Average trajectory reward is -175.64\n",
            "End of rollout 5: Average trajectory reward is -193.53\n",
            "End of rollout 6: Average trajectory reward is -196.70\n",
            "End of rollout 7: Average trajectory reward is -201.39\n",
            "End of rollout 8: Average trajectory reward is -201.41\n",
            "End of rollout 9: Average trajectory reward is -186.89\n",
            "End of rollout 10: Average trajectory reward is -201.89\n",
            "End of rollout 11: Average trajectory reward is -180.45\n",
            "End of rollout 12: Average trajectory reward is -236.35\n",
            "End of rollout 13: Average trajectory reward is -233.64\n",
            "End of rollout 14: Average trajectory reward is -235.84\n",
            "End of rollout 15: Average trajectory reward is -229.54\n",
            "End of rollout 16: Average trajectory reward is -194.51\n",
            "End of rollout 17: Average trajectory reward is -228.78\n",
            "End of rollout 18: Average trajectory reward is -231.48\n",
            "End of rollout 19: Average trajectory reward is -233.50\n",
            "End of rollout 20: Average trajectory reward is -208.42\n",
            "End of rollout 21: Average trajectory reward is -223.50\n",
            "End of rollout 22: Average trajectory reward is -243.76\n",
            "End of rollout 23: Average trajectory reward is -231.68\n",
            "End of rollout 24: Average trajectory reward is -212.48\n",
            "End of rollout 25: Average trajectory reward is -236.23\n",
            "End of rollout 26: Average trajectory reward is -254.96\n",
            "End of rollout 27: Average trajectory reward is -263.14\n",
            "End of rollout 28: Average trajectory reward is -260.74\n",
            "End of rollout 29: Average trajectory reward is -223.38\n",
            "End of rollout 30: Average trajectory reward is -224.39\n",
            "End of rollout 31: Average trajectory reward is -274.44\n",
            "End of rollout 32: Average trajectory reward is -237.83\n",
            "End of rollout 33: Average trajectory reward is -268.01\n",
            "End of rollout 34: Average trajectory reward is -237.73\n",
            "End of rollout 35: Average trajectory reward is -230.14\n",
            "End of rollout 36: Average trajectory reward is -223.95\n",
            "End of rollout 37: Average trajectory reward is -242.29\n",
            "End of rollout 38: Average trajectory reward is -224.46\n",
            "End of rollout 39: Average trajectory reward is -236.32\n",
            "End of rollout 40: Average trajectory reward is -243.97\n",
            "End of rollout 41: Average trajectory reward is -217.80\n",
            "End of rollout 42: Average trajectory reward is -219.34\n",
            "End of rollout 43: Average trajectory reward is -190.34\n",
            "End of rollout 44: Average trajectory reward is -204.40\n",
            "End of rollout 45: Average trajectory reward is -241.00\n",
            "End of rollout 46: Average trajectory reward is -209.95\n",
            "End of rollout 47: Average trajectory reward is -222.61\n",
            "End of rollout 48: Average trajectory reward is -228.81\n",
            "End of rollout 49: Average trajectory reward is -199.23\n",
            "End of rollout 50: Average trajectory reward is -215.98\n",
            "End of rollout 51: Average trajectory reward is -215.14\n",
            "End of rollout 52: Average trajectory reward is -220.73\n",
            "End of rollout 53: Average trajectory reward is -229.03\n",
            "End of rollout 54: Average trajectory reward is -205.54\n",
            "End of rollout 55: Average trajectory reward is -223.56\n",
            "End of rollout 56: Average trajectory reward is -212.74\n",
            "End of rollout 57: Average trajectory reward is -235.72\n",
            "End of rollout 58: Average trajectory reward is -225.59\n",
            "End of rollout 59: Average trajectory reward is -220.65\n",
            "End of rollout 60: Average trajectory reward is -211.52\n",
            "End of rollout 61: Average trajectory reward is -241.92\n",
            "End of rollout 62: Average trajectory reward is -243.63\n",
            "End of rollout 63: Average trajectory reward is -226.55\n",
            "End of rollout 64: Average trajectory reward is -246.16\n",
            "End of rollout 65: Average trajectory reward is -208.61\n",
            "End of rollout 66: Average trajectory reward is -184.75\n",
            "End of rollout 67: Average trajectory reward is -200.34\n",
            "End of rollout 68: Average trajectory reward is -223.82\n",
            "End of rollout 69: Average trajectory reward is -223.80\n",
            "End of rollout 70: Average trajectory reward is -181.81\n",
            "End of rollout 71: Average trajectory reward is -242.50\n",
            "End of rollout 72: Average trajectory reward is -216.01\n",
            "End of rollout 73: Average trajectory reward is -222.44\n",
            "End of rollout 74: Average trajectory reward is -206.62\n",
            "End of rollout 75: Average trajectory reward is -240.80\n",
            "End of rollout 76: Average trajectory reward is -192.90\n",
            "End of rollout 77: Average trajectory reward is -218.89\n",
            "End of rollout 78: Average trajectory reward is -196.84\n",
            "End of rollout 79: Average trajectory reward is -204.53\n",
            "End of rollout 80: Average trajectory reward is -182.10\n",
            "End of rollout 81: Average trajectory reward is -214.97\n",
            "End of rollout 82: Average trajectory reward is -187.11\n",
            "End of rollout 83: Average trajectory reward is -166.60\n",
            "End of rollout 84: Average trajectory reward is -179.70\n",
            "End of rollout 85: Average trajectory reward is -187.09\n",
            "End of rollout 86: Average trajectory reward is -172.88\n",
            "End of rollout 87: Average trajectory reward is -174.86\n",
            "End of rollout 88: Average trajectory reward is -189.42\n",
            "End of rollout 89: Average trajectory reward is -203.51\n",
            "End of rollout 90: Average trajectory reward is -176.68\n",
            "End of rollout 91: Average trajectory reward is -159.30\n",
            "End of rollout 92: Average trajectory reward is -183.94\n",
            "End of rollout 93: Average trajectory reward is -166.29\n"
          ]
        }
      ]
    }
  ]
}